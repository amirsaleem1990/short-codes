# tokenize and remove puncuations

from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
df["tokens"] = df["text"].apply(tokenizer.tokenize)


# count all words(tokens) in the data
from collections import Counter
count_all_words = Counter(all_words)
#return a dict where each key is a unique word in data and each value count of that word accuring in whole data.. 
count_all_words
# {'just': 459,
         'happened': 32,
         'a': 3109,
         'terrible': 14,
         'car': 133,
         'crash': 167,
         'our': 151,
         'deeds': 2,
         'are': 600,
         'the': 4621,
         'reason': 29}

#give only most commeon 100 words in data:
count_all_words.most_common(100)

# [('the', 4621),
 ('a', 3109),
 ('to', 2837),
 ('in', 2808),
 ('of', 2610),
 ('i', 2511),
 ('and', 2023),
 ('s', 1403),
 ('is', 1392),
 ('you', 1287),
 ('for', 1245),
 ('on', 1238),
 ('it', 1141),
 ('my', 976),
 ('that', 853),
 ('with', 797),
 ('by', 777),
 .....
 .....]
 